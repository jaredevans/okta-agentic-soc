# Okta Agentic SOC

This project is an AI-powered, agent-based system designed to automate the security operations center (SOC) workflow for Okta. It ingests Okta system logs, detects potential threats, assesses risk, develops response plans, and suggests commands for remediation.

## Features

- **Automated Log Ingestion**: Fetches Okta system logs from a specified time window.
- **Threat Detection**: Includes built-in detectors for common threats such as:
  - **Impossible Travel**: Detects logins from geographically distant locations in a short period.
  - **Failed Login Burst**: Identifies brute-force attempts by monitoring rapid failed login events.
- **AI-Powered Analysis**: Utilizes Large Language Models (LLMs) for:
  - **Risk Assessment**: Analyzes findings to determine severity, impact, and a risk score, promoting significant findings to incidents.
  - **Response Planning**: Generates step-by-step response plans for security incidents.
  - **Orchestration**: A router agent dynamically decides which agents to run based on the context.
- **Command Generation**: Suggests `curl` commands for executing response plan steps, such as suspending a user or revoking sessions.
- **Extensible Architecture**: Easily add new detectors and agents to enhance capabilities.
- **Simple CLI**: Provides a straightforward command-line interface to run the pipeline and view results.

## How It Works

The system operates in a two-stage pipeline orchestrated by a central **Router Agent**:

1.  **Stage 1: Detection and Analysis**
    -   **Ingestion**: Okta system logs are fetched for a given time period.
    -   **Detection**: The **Detector Agent** runs all registered detectors (e.g., `ImpossibleTravelDetector`, `FailedLoginBurstDetector`) against the logs to produce `findings`.
    -   **Risk Analysis**: The **Risk Agent** uses an LLM to evaluate each finding, assign a risk score, and decide whether to promote it to a `security incident`.

2.  **Stage 2: Incident Response**
    -   **Planning**: For each new incident, the **Planner Agent** uses an LLM to generate a structured `response plan` with clear steps and rationale.
    -   **Command Generation**: The **Command Agent** takes the response plan and generates corresponding API commands (e.g., `curl` commands for the Okta API) for a human analyst to review and execute.

All artifacts—findings, incidents, plans, and commands—are saved to `.jsonl` files in the `data/` directory.

## Getting Started

### Prerequisites

-   Python 3.11+
-   Access to an Okta organization with API access.
-   Access to a running LLM that supports the OpenAI API format.

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/okta-agentic-soc.git
    cd okta-agentic-soc
    ```

2.  **Install dependencies:**
    ```bash
    pip install .
    ```

3.  **Configure Environment Variables:**
    Create a `.env` file in the project root directory and add the following variables.

    ```env
    # Your Okta organization URL (e.g., https://dev-12345.okta.com)
    OKTA_ORG_URL="your_okta_org_url"

    # Your Okta API token
    OKTA_API_TOKEN="your_okta_api_token"

    # The base URL of your LLM API endpoint
    LLM_BASE_URL="http://localhost:1234/v1"

    # The model name to use
    LLM_MODEL="your-llm-model-name"
    ```

## Usage

The project provides a command-line interface through `okta-soc`.

### Run the Full Pipeline

To ingest Okta logs from the last N hours and run the entire detection and response pipeline:

```bash
okta-soc --hours 24
```

This command will:
1.  Fetch Okta logs from the last 24 hours.
2.  Run the detection and analysis stage.
3.  Run the incident response stage for any created incidents.
4.  Save all results to the `data/` directory.

### View Results

To view all the findings, incidents, plans, and commands generated by the pipeline, use the `show-all` command:

```bash
okta-soc show-all
```

This will pretty-print the contents of the `.jsonl` files in the `data/` directory to your console.